## 第三章　失敗しないLLM導入のためのチェックリスト

### 敵対的リスク

敵対的リスクは、相手として攻撃者だけでなく競合他社も含んでいます。

- 競合他社がどのようにAIに投資しているかを精査してください。AIの導入にはリスクがありますが、将来のマーケットシェアに影響を与えるビジネス上のメリットもあります。
- 生成AIの出現によりセキュリティ保護がもはや有効に機能しなくなった可能性のある現行の管理策を調査する。たとえば、パスワードのリセットに音声認識を利用している場合など、従来は困難だった攻撃が新たな生成AIの出現により、容易に行えるようになりました。
- インシデント対応計画と手引きを更新し、生成AIによる攻撃やAI・ML特有のインシデントへの対応を強化する必要があります。

### 脅威のモデリング

脅威を特定しプロセスとセキュリティ防御を検討するために、脅威モデリングを実施することを強く推奨します。脅威モデリングは、アプリケーション、ソフトウェア、システムのセキュリティに関する合理的な意志決定を行うための体系的で、反復可能な一連のプロセスです。生成AIを使った攻撃に対する脅威モデリングやLLMを導入する直前の脅威モデリングは、リスクを特定・軽減し、データを保護し、プライバシーを保護し、ビジネス内の安全でコンプライアンスに準拠した統合を確実にする、最も費用対効果の高い方法です。

- 攻撃者は、組織、従業員、経営陣、ユーザーに対する攻撃をどのようにして加速させるのでしょうか？生成AIを使用した大規模で高度な「なりすまし攻撃」を予測する必要があります。スピア・フィッシング攻撃（註1）にLLMを使うことにより、非常に巧妙で、狙う人・組織を絞りこむことができるようになります。
- 企業の顧客やクライアントへの攻撃に、スプーフィング（註2）や生成AIが生成したコンテンツを通じて、生成AIがどのように利用される可能性があるでしょうか？
- あなたのLLMソリューションへ有害または悪意のある入力やクエリがあった場合、それを検出し、未然に防ぐことができますか？
- LLMのすべての信頼境界で、内部のシステムとデータベースを、外部からの不正アクセスから保護できますか？
- 許可されたユーザーによる誤用を防ぐために、組織内部からの脅威の緩和策を講じていますか？
- 知的財産を保護するために、独自のモデルやデータへの不正アクセスを防止できますか？
- コンテンツ・フィルタリングを自動化することで、有害または不適切なコンテンツの生成を防ぐことができますか？

【註】
1. スピア・フィッシング攻撃
  特定の個人や組織を狙った偽メールによるフィッシング攻撃で、目的はログイン・パスワードなどの情報を盗む、あるいは、狙ったデバイスにマルウエアを感染させること
2. スプーフィング
  ハッカーが金融機関の口座などへのアクセス権限を得るために、他人のデバイスやユーザーになりすますこと

### AI資産目録

AI資産目録は、社内で開発されたソリューションと外部またはサードパーティのソリューションの両方に適用する必要があります。

- 既存のAIサービス、ツール、所有者を記録し、資産管理で固有のタグを指定すること。
- ソフトウェア部品表（SBOM）は、アプリケーションに関連するすべてのソフトウェア部品、依存関係、およびメタデータの包括的なリストです。ソフトウェア部品表（SBOM）にAIを含むソフトウエア部品を含めること。
- AIに使用するデータソースとデータの機密性（保護、機密、公開）を記録管理すること。
- 現在の攻撃サーフェスのリスクを判断するために、設置されたAIソリューションのペンテスト（註3）またはレッドチーム（註4）が必要かどうかを確認する。
- AIソリューションの導入時検査プロセスを策定する。
- SBoMの要件に従い、熟練したIT管理スタッフを社内または社外から確保する。

【註】
3. ペンテスト（ペネトレーションテスト）
  ネットワーク、PC・サーバーやシステムの脆弱性を検証するテスト手法の1つ。 実際にシステムに攻撃を仕掛け侵入を試みることから、「侵入テスト」とも呼ばれる。
4. レッドチーム
  セキュリティの脆弱性を検証するためなどの目的で設置された、その組織とは独立したチームのことで、対象組織に敵対したり、攻撃したりといった役割を担う。

### AIセキュリティとプライバシー研修

- 従業員と積極的に関わり、計画されているLLM導入についての従業員の懸念を理解し対処する。
- オープンで透明性のあるコミュニケーション文化を確立し、組織のプロセス、システム、従業員の管理とサポート、顧客との関係における予測AI・生成AIの使用と管理、リスクへの対処法について意思疎通をはかる。
- 倫理、責任、法的問題（保証、ライセンス、著作権）についての理解をすべてのユーザーに徹底する。
- セキュリティ意識向上トレーニングを更新し、生成AI関連の脅威を含める。ボイスクローニング、イメージクローニング、スピアフィッシング攻撃の増加を前提としたものとする。
- 生成AIソリューションを採用する時には、DevOpsとサイバーセキュリティの両方のトレーニングを行い、AIのサイバーセキュリティとセキュリティを保証するためのデプロイメント・パイプラインを含める。

### ビジネスケースの確立

確固としたビジネスケースは、AIソリューションのビジネス価値を判断し、リスクと価値のバランスを取り、投資対効果を評価・検証するために不可欠です。ビジネスケースの例を以下に示します。

- 顧客体験の向上
- 運用効率の向上
- より良い知識管理
- イノベーションの強化
- 市場調査と競合分析
- 文書作成、翻訳、要約、分析

### ガバナンス

LLMにおけるコーポレート・ガバナンスは、組織に透明性と説明責任を提供するために必要です。デジタルプロセスへの防衛にスピーディに対応するために、テクノロジーとビジネスが目的とするユースケースに精通しているAIプラットフォーム、並びに、プロセス管理者を置くことは、十分に確立された企業の必要条件です。

- 組織のAI RACIチャート（責任者、説明責任者、相談役、報告先）の確立
- AIリスク、リスク評価、組織内のガバナンス責任を文書化し、担当者を割り当てます。
- 技術的実装を含め、データの分類と使用制限に関するデータ管理ポリシーを確立します。モデルは、システムのあらゆるユーザの最小アクセス・レベルに分類されたデータのみを使用すべきです。例えば、データ保護ポリシーを更新して、保護されたデータや機密データをビジネス管理されていないツールに入力しないことを明記します。
- 確立されたポリシー（例：善行基準、データ保護、ソフトウェア使用）に支えられたAIポリシーを作成します。
- 様々な生成AIツールを従業員が使用する際、使用してよいかどうかを占めす一覧表を公開します。
- 組織が LLM 生成モデルで使用するデータのソースと管理を文書化する。


### 法務

AIの法規定は決まっていない点が多く、非常に大きなコストがかかる可能性があります。IT、セキュリティ、法務の三部門の連携は、不確定部分を特定し、不明瞭な決定に対処するために不可欠です。

- 製品保証を製品開発の流れの中で明確にし、AIに関わる製品保証の責任者を割り当てます。
- 生成AIを考慮して、既存の利用規約を見直し、更新します。
- AI EULA契約書のレビューを行います。生成AIプラットフォームのエンドユーザーライセンス契約は、ユーザープロンプトの扱い方、アウトプットの権利と所有権、データプライバシー、コンプライアンス、責任の所在、プライバシー、アウトプットの使用制限など、多岐にわたり大きく異なります。
- AIが生成したコンテンツによる盗作、偏見の伝播、知的財産の侵害に関連する責任を組織が負うことを防ぐために、顧客向けのEULA、エンドユーザー契約を変更します。
- コード開発に使用されている既存のAI支援ツールの見直し。チャットボットが書いたコードを製品に使用する場合、その製品に関する企業の所有権を脅かす可能性があります。すなわち、生成されたコードの状態や保護、生成されたコードを使用する権利を誰が保持しているかが問題になる可能性があります。
- 知的財産に対するリスクの確認。チャットボットによって生成された知的財産は、生成プロセスが、著作権、商標、または特許保護の対象となるデータを使用する場合、危険にさらされる可能性があります。AI製品が侵害する素材を使用した場合、AIのアウトプットにリスクが生じ、知的財産の侵害につながる可能性があります。
- 免責条項のある契約を見直しましょう。免責条項は、責任につながる事象の責任を、その事象についてより過失があった人、あるいはその事象を阻止する可能性があったがそうしなかった人に押し付けようとするものです。責任につながる事象を引き起こしたのは、AIの提供者か、あるいはその利用者かを判断するための基準を設定する必要があります。
- AIシステムに起因する潜在的な傷害や物的損害に対する賠償責任を検討する。
- 従来のDirectors&Officers(D&O賠償責任保険や商業賠償責任保険は、AIの利用を完全に保護するには不十分である可能性が高いので、保険の見直しを行います。
- 著作権に関する問題の特定。著作権には人間が著作者であること必要です。LLMツールが悪用された場合、組織は剽窃、偏見の伝播、知的財産権侵害の責任を負う可能性もあります。
- ベンダーが開発または提供するサービスに関して、生成AIの適切な使用に関する契約条項を含めること。
- 権利行使が問題となる可能性がある場合、または知的財産権侵害が懸念される場合、従業員または請負業者に対する生成AIツールの使用を制限または禁止すること。
- 従業員管理や雇用の評価にAIを使用すると、差別待遇クレームや差別的影響クレームの原因となる可能性があります。
- AIソリューションが適切な同意や承認なしに機密情報を収集したり共有したりしないことを確認してください。

### 規制（米国、カナダ）

EUのAI法は最初の包括的なAI法になると予想されていますが、適用されるのは早くても2025年の見込みです。EUの一般データ保護規則（GDPR）はAIを特に取り上げていませんが、データ収集、データセキュリティ、公平性と透明性、正確性と信頼性、説明責任に関する規則を含んでおり、生成AIの利用に影響を与える可能性があります。米国では、AI規制はより広範な消費者プライバシー法の中に含まれています。米国では10の州で法律が成立しているか、2023年末までに施行される予定です。

カナダは、[生成AIシステムの開発責任と管理に関する任意の行動綱領（Voluntary Code of Conduct on the Responsible Development and Management of Advanced Generative AI Systems）](https://ised-isde.canada.ca/site/ised/en/voluntary-code-conduct-responsible-development-and-management-advanced-generative-ai-systems)を公開しています。
一方、[Artificial Intelligence and Data Act (AIDA) ](https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act)には、それ以上強い規制が盛り込まれる見込みです。

米国雇用機会均等委員会（EEOC）、消費者金融保護局（CFPB）、連邦取引委員会（FTC）、米国司法省公民権局（DOJ）などの連邦機関は、雇用の公平性を厳しく監視しています。

- 国、州、またはその他の政府固有のAIコンプライアンス要件を決定する。
- 従業員の電子的監視および雇用関連の自動意思決定システムの制限に関するコンプライアンス要件の決定（バーモント州、カリフォルニア州、メリーランド州、ニューヨーク州、ニュージャージー州）
- 顔認識とAIビデオ分析に必要な同意に関するコンプライアンス要件の決定（イリノイ州、メリーランド州、ワシントン州、バーモント州）
- 従業員の採用や管理に使用されている、または検討されているAIツールを確認します。
- ベンダーが適用されるAIに関する法律やベストプラクティスを遵守していることを確認します。
- 人事採用プロセスでAIを使用した製品を使っている場合、文書化します。モデルがどのように訓練され、どのように監視され、差別や偏見を避けるために修正されたものを追跡しているかを確認します。
- どのような宿泊オプションが含まれているかを尋ね、記録しておきましょう。
- ベンダーが機密データを収集しているかどうかを尋ね、文書化してください。
- ベンダーやツールがどのようにデータを保存・削除し、入社前の顔認識やビデオ分析ツールの使用を規制しているかを尋ねてください。
- AIに関連して、コンプライアンス上の問題が発生する可能性のある他の組織固有の規制要件を確認してください。例えば、1974年従業員退職所得保障法（Employee Retirement Income Security Act of 1974）には、チャットボットでは対応できない可能性のある退職金制度のための教育義務要件があります。

### 大規模言語モデルソリューションの使用または実装

- 脅威モデル LLM のコンポーネントとアーキテクチャの信頼境界を見極める。
- データ・セキュリティ、データが機密性に基づいてどのように分類され、保護されているかを検証してください。ユーザの権限はどのように管理され、どのような保護措置が取られていますか？
- アクセス・コントロール、最小権限アクセス・コントロールの実施、徹底的な防衛策の実施
- トレーニングパイプラインセキュリティは、トレーニングデータガバナンス、パイプライン、モデル、アルゴリズムに関する厳格な管理を必要とします。
- 入力と出力のセキュリティでは、入力の検証方法と、出力がどのようにフィルタリングされ、サニタイズされ、承認されるかを評価します。
- 監視と応答、ワークフロー、監視、応答をマッピングし、自動化、ロギング、監査を理解します。監査記録の安全性を確認します。
- 製品リリースプロセスにおけるアプリケーションテスト、ソースコードレビュー、脆弱性評価、レッドチーミングの実施。
- LLMモデルまたはサプライチェーンに脆弱性が存在するどうかチェックします。
- プロンプト・インジェクション、機密情報の流出、プロセス操作など、LLMソリューションに対する脅威や攻撃の影響を調査します。
- モデルポイズニング（註5）、不適切なデータの取り扱い、サプライチェーン攻撃、モデルの盗難など、LLMモデルに対する攻撃や脅威の影響を調査します。
- サプライチェーンセキュリティ、第三者監査、侵入テスト、第三者プロバイダーのコードレビューを依頼します。(初期および継続的に）
- インフラストラクチャ・セキュリティ、ベンダーがレジリエンス・テストを実施する頻度は？可用性、スケーラビリティ、パフォーマンスに関するSLAはどうなっていますか。
- インシデント対応手順書を更新し、卓上演習にLLMインシデントを盛り込みます。
- 生成AIのサイバーセキュリティを他のアプローチと比較するベンチマーク指標を策定し、または、拡張して、期待される生産性の向上を測定します。

【註】
5. モデルポイズニング
  攻撃者がモデルの訓練データに不正なデータを混入し、モデルが訓練されること

### 試験、評価、検証、妥当性確認（TEVV）

NIST AIフレームワークでは、AIシステム運用者、ドメインエキスパート、AI設計者、ユーザー、製品開発者、評価者、監査人を含む、AIライフサイクル全体にわたる継続的なTEVVプロセスを推奨しています。TEVVには、システムの妥当性確認、統合、テスト、再較正、AIシステムのリスクや変更をナビゲートするための定期的な更新のための継続的なモニタリングなど、さまざまなタスクが含まれます。

- AIモデルのライフサイクルを通じて、継続的なテスト、評価、検証、妥当性確認を確立します。
- AIモデルの機能性、セキュリティ、信頼性、堅牢性に関する定期的な経営指標と最新情報の提供。

### モデル・カードとリスク・カード

モデルカードとリスクカードは、大規模言語モデル（LLM）の透明性、説明責任、および倫理的な導入を向上させるための基本要素です。
モデル・カードは、AIシステムの設計、能力、制約に関する標準化されたドキュメントを提供することで、ユーザがAIシステムを理解し、信頼できるようにします。
リスクカードは、バイアス、プライバシーの問題、セキュリティの脆弱性など、潜在的な悪影響をオープンに扱うことでこれを補い、危害防止への積極的なアプローチを促します。これらの文書は、AIの社会的影響を慎重に扱い、強調して対応する土壌を確立するため、開発者、ユーザー、規制当局、倫理学者にとって重要です。これらのカードは、モデルを作成した組織によって開発され、維持されており、AI技術が倫理基準と法的要件を満たすことを保証し、AIエコシステムにおける責任ある研究と展開を可能にする上で重要な役割を果たしています。

モデルカードは、MLモデルに関連する以下のような属性を含んでいます。

  - モデルの詳細：モデルに関する基本情報、すなわち、名前、バージョン、タイプ（ニューラルネットワーク、決定木など）、および意図された使用方法。
  - モデルのアーキテクチャ：層の数とタイプ、活性化関数、その他の主要なアーキテクチャの選択など、モデルの構造の説明を含みます。
  - トレーニングデータと手法：データセットのサイズ、データソース、使用した前処理やデータ補強技術など、モデルの学習に使用したデータに関する情報。また、使用されたオプティマイザ、損失関数、チューニングされたハイパーパラメータなど、トレーニング手法の詳細も含まれます。
  - パフォーマンス測定基準：精度、正確度、再現率、F1スコアなど、さまざまな測定基準におけるモデルのパフォーマンスに関する情報。また、データの異なるサブセットでのモデルのパフォーマンスに関する情報も含まれることもあります。
  - 潜在的なバイアスと限界：不均衡なトレーニングデータ、オーバーフィッティング、モデルの予測におけるバイアスなど、モデルの潜在的なバイアスや制限をリストアップします。また、新しいデータへの適応能力や特定の使用例への適合性など、モデルの限界に関する情報も含まれます。
  - 責任あるAIへの配慮：プライバシーに関する懸念、公平性、透明性、またはモデルの使用による潜在的な社会的影響など、モデルに関連する倫理的または責任あるAIの考慮事項。また、モデルのさらなるテスト、検証、モニタリングに関する推奨事項が含まれる場合もあります。

モデルカードに含まれる正確な機能は、モデルのコンテキストと使用目的によって異なる可能性がありますが、その目的は、機械学習モデルの作成と展開に公開性と説明責任を与えることです。

- モデルカードの確認
- リスクカードがある場合はそれを確認
- サードパーティを通じて使用されるモデルを含む、あらゆる導入モデルのモデルカードを追跡し、維持するプロセスを確立します。

### RAG：大規模言語モデルの最適化

ファインチューニングは、事前に訓練されたモデルを最適化するための手法で、既存のモデルを新しいドメイン固有のデータで再学習させ、タスクやアプリケーションのパフォーマンスに合わせて行います。ファインチューニングにはコストがかかりますが、パフォーマンス向上には不可欠です。

検索補強型生成（Retrieval-Augmented Generation, RAG）は、最新の利用可能な知識ソースから適切なデータを検索することにより、大規模な言語モデルの能力を最適化し補強する、より効果的な方法として発展してきました。RAGは特定のドメイン用にカスタマイズすることができ、ドメイン固有の情報の検索を最適化し、特殊な分野のニュアンスに合わせて生成プロセスを調整します。RAGは、LLM最適化のための、より効率的で透明性の高い手法と考えられており、特にラベル付きデータの収集が限られていたり、高価であったりする場合に適しています。また、RAGの利点の一つは、検索段階で新しい情報を継続的に更新することができるため、継続的な学習をサポートすることです。

RAGの実装はいくつかのステップがあります。まず、埋め込み情報モデルの導入から始まり、知識ライブラリの索引付け、クエリ処理のための最も関連性の高い文書の検索まで、いくつかの重要なステップを含みます。関連するコンテキストの効率的な検索は、文書埋め込み情報の保存とクエリに使用されるベクトルデータベースを使って行います。

#### RAG 参照・リンク
- [検索拡張世代（RAG）とLLM：例](https://vitalflux.com/retrieval-augmented-generation-rag-llm-examples/)
- [12 RAGの問題点と解決案](https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c)

### AIレッドチーム

AIレッドチームとは、攻撃者が悪用できる脆弱性が存在しないことを確認するために、AIシステムを敵対的に攻撃するテストシミュレーションです。これは、バイデン政権を含む多くの規制機関やAI管理機関によって推奨されています。しかし、レッドチーミングだけでは、AIシステムに関連するすべての実害を検証する包括的な解決策とはなりません。アルゴリズムによる影響評価や外部監査など、他の形式のテスト、評価、検証、妥当性確認と組み合わせる必要があります。

- AIモデルとアプリケーションの標準的なプラクティスとして、レッドチームテストを導入します。